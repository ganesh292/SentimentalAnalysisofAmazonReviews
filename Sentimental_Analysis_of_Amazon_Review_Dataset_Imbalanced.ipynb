{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentimental_Analysis_of_Amazon_Review_Dataset_Imbalanced.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "eAIyoB_-fG9g"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "ZrT1FaZREZ03",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Install Packages"
      ]
    },
    {
      "metadata": {
        "id": "vG9S4vusYNAr",
        "colab_type": "code",
        "outputId": "cd704134-e0d7-4823-c6a5-53eeaa278aab",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        }
      },
      "cell_type": "code",
      "source": [
        "!pip install -U -q PyDrive\n",
        "!pip install -q glove_python\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize,word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "nltk.download('universal_tagset')\n",
        "nltk.download('wordnet')\n",
        "lemmatizer = nltk.WordNetLemmatizer()\n",
        "\n",
        "from bs4 import BeautifulSoup  \n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
        "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import chi2\n",
        "\n",
        "from sklearn.svm import SVC\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K    1% |▎                               | 10kB 28.5MB/s eta 0:00:01\r\u001b[K    2% |▋                               | 20kB 6.5MB/s eta 0:00:01\r\u001b[K    3% |█                               | 30kB 9.1MB/s eta 0:00:01\r\u001b[K    4% |█▎                              | 40kB 6.1MB/s eta 0:00:01\r\u001b[K    5% |█▋                              | 51kB 7.3MB/s eta 0:00:01\r\u001b[K    6% |██                              | 61kB 8.6MB/s eta 0:00:01\r\u001b[K    7% |██▎                             | 71kB 9.7MB/s eta 0:00:01\r\u001b[K    8% |██▋                             | 81kB 10.8MB/s eta 0:00:01\r\u001b[K    9% |███                             | 92kB 11.9MB/s eta 0:00:01\r\u001b[K    10% |███▎                            | 102kB 9.8MB/s eta 0:00:01\r\u001b[K    11% |███▋                            | 112kB 9.9MB/s eta 0:00:01\r\u001b[K    12% |████                            | 122kB 13.2MB/s eta 0:00:01\r\u001b[K    13% |████▎                           | 133kB 13.0MB/s eta 0:00:01\r\u001b[K    14% |████▋                           | 143kB 22.3MB/s eta 0:00:01\r\u001b[K    15% |█████                           | 153kB 22.7MB/s eta 0:00:01\r\u001b[K    16% |█████▎                          | 163kB 22.6MB/s eta 0:00:01\r\u001b[K    17% |█████▋                          | 174kB 22.8MB/s eta 0:00:01\r\u001b[K    18% |██████                          | 184kB 23.3MB/s eta 0:00:01\r\u001b[K    19% |██████▎                         | 194kB 23.4MB/s eta 0:00:01\r\u001b[K    20% |██████▋                         | 204kB 48.7MB/s eta 0:00:01\r\u001b[K    21% |███████                         | 215kB 28.8MB/s eta 0:00:01\r\u001b[K    22% |███████▎                        | 225kB 29.1MB/s eta 0:00:01\r\u001b[K    23% |███████▋                        | 235kB 30.3MB/s eta 0:00:01\r\u001b[K    24% |████████                        | 245kB 30.0MB/s eta 0:00:01\r\u001b[K    25% |████████▎                       | 256kB 30.3MB/s eta 0:00:01\r\u001b[K    26% |████████▋                       | 266kB 27.7MB/s eta 0:00:01\r\u001b[K    27% |█████████                       | 276kB 28.3MB/s eta 0:00:01\r\u001b[K    29% |█████████▎                      | 286kB 28.0MB/s eta 0:00:01\r\u001b[K    30% |█████████▋                      | 296kB 27.8MB/s eta 0:00:01\r\u001b[K    31% |██████████                      | 307kB 30.9MB/s eta 0:00:01\r\u001b[K    32% |██████████▎                     | 317kB 61.4MB/s eta 0:00:01\r\u001b[K    33% |██████████▋                     | 327kB 61.5MB/s eta 0:00:01\r\u001b[K    34% |███████████                     | 337kB 63.8MB/s eta 0:00:01\r\u001b[K    35% |███████████▎                    | 348kB 55.7MB/s eta 0:00:01\r\u001b[K    36% |███████████▋                    | 358kB 55.6MB/s eta 0:00:01\r\u001b[K    37% |████████████                    | 368kB 67.8MB/s eta 0:00:01\r\u001b[K    38% |████████████▎                   | 378kB 69.8MB/s eta 0:00:01\r\u001b[K    39% |████████████▋                   | 389kB 71.5MB/s eta 0:00:01\r\u001b[K    40% |█████████████                   | 399kB 71.3MB/s eta 0:00:01\r\u001b[K    41% |█████████████▎                  | 409kB 70.5MB/s eta 0:00:01\r\u001b[K    42% |█████████████▋                  | 419kB 71.2MB/s eta 0:00:01\r\u001b[K    43% |██████████████                  | 430kB 70.8MB/s eta 0:00:01\r\u001b[K    44% |██████████████▎                 | 440kB 70.5MB/s eta 0:00:01\r\u001b[K    45% |██████████████▋                 | 450kB 64.5MB/s eta 0:00:01\r\u001b[K    46% |███████████████                 | 460kB 54.8MB/s eta 0:00:01\r\u001b[K    47% |███████████████▎                | 471kB 54.4MB/s eta 0:00:01\r\u001b[K    48% |███████████████▋                | 481kB 54.0MB/s eta 0:00:01\r\u001b[K    49% |████████████████                | 491kB 54.2MB/s eta 0:00:01\r\u001b[K    50% |████████████████▎               | 501kB 54.8MB/s eta 0:00:01\r\u001b[K    51% |████████████████▋               | 512kB 51.3MB/s eta 0:00:01\r\u001b[K    52% |█████████████████               | 522kB 50.2MB/s eta 0:00:01\r\u001b[K    53% |█████████████████▎              | 532kB 49.0MB/s eta 0:00:01\r\u001b[K    54% |█████████████████▋              | 542kB 48.9MB/s eta 0:00:01\r\u001b[K    55% |██████████████████              | 552kB 62.1MB/s eta 0:00:01\r\u001b[K    57% |██████████████████▎             | 563kB 75.7MB/s eta 0:00:01\r\u001b[K    58% |██████████████████▋             | 573kB 76.2MB/s eta 0:00:01\r\u001b[K    59% |███████████████████             | 583kB 76.9MB/s eta 0:00:01\r\u001b[K    60% |███████████████████▎            | 593kB 77.0MB/s eta 0:00:01\r\u001b[K    61% |███████████████████▋            | 604kB 77.3MB/s eta 0:00:01\r\u001b[K    62% |████████████████████            | 614kB 85.4MB/s eta 0:00:01\r\u001b[K    63% |████████████████████▎           | 624kB 86.6MB/s eta 0:00:01\r\u001b[K    64% |████████████████████▋           | 634kB 92.0MB/s eta 0:00:01\r\u001b[K    65% |█████████████████████           | 645kB 92.4MB/s eta 0:00:01\r\u001b[K    66% |█████████████████████▎          | 655kB 91.5MB/s eta 0:00:01\r\u001b[K    67% |█████████████████████▋          | 665kB 58.6MB/s eta 0:00:01\r\u001b[K    68% |██████████████████████          | 675kB 57.0MB/s eta 0:00:01\r\u001b[K    69% |██████████████████████▎         | 686kB 57.4MB/s eta 0:00:01\r\u001b[K    70% |██████████████████████▋         | 696kB 57.5MB/s eta 0:00:01\r\u001b[K    71% |███████████████████████         | 706kB 46.1MB/s eta 0:00:01\r\u001b[K    72% |███████████████████████▎        | 716kB 46.0MB/s eta 0:00:01\r\u001b[K    73% |███████████████████████▋        | 727kB 36.3MB/s eta 0:00:01\r\u001b[K    74% |████████████████████████        | 737kB 34.5MB/s eta 0:00:01\r\u001b[K    75% |████████████████████████▎       | 747kB 34.4MB/s eta 0:00:01\r\u001b[K    76% |████████████████████████▋       | 757kB 34.1MB/s eta 0:00:01\r\u001b[K    77% |████████████████████████▉       | 768kB 43.1MB/s eta 0:00:01\r\u001b[K    78% |█████████████████████████▏      | 778kB 43.9MB/s eta 0:00:01\r\u001b[K    79% |█████████████████████████▌      | 788kB 43.6MB/s eta 0:00:01\r\u001b[K    80% |█████████████████████████▉      | 798kB 43.6MB/s eta 0:00:01\r\u001b[K    81% |██████████████████████████▏     | 808kB 52.5MB/s eta 0:00:01\r\u001b[K    82% |██████████████████████████▌     | 819kB 52.7MB/s eta 0:00:01\r\u001b[K    83% |██████████████████████████▉     | 829kB 76.6MB/s eta 0:00:01\r\u001b[K    85% |███████████████████████████▏    | 839kB 85.3MB/s eta 0:00:01\r\u001b[K    86% |███████████████████████████▌    | 849kB 87.3MB/s eta 0:00:01\r\u001b[K    87% |███████████████████████████▉    | 860kB 75.7MB/s eta 0:00:01\r\u001b[K    88% |████████████████████████████▏   | 870kB 70.7MB/s eta 0:00:01\r\u001b[K    89% |████████████████████████████▌   | 880kB 70.3MB/s eta 0:00:01\r\u001b[K    90% |████████████████████████████▉   | 890kB 70.9MB/s eta 0:00:01\r\u001b[K    91% |█████████████████████████████▏  | 901kB 70.8MB/s eta 0:00:01\r\u001b[K    92% |█████████████████████████████▌  | 911kB 73.2MB/s eta 0:00:01\r\u001b[K    93% |█████████████████████████████▉  | 921kB 74.0MB/s eta 0:00:01\r\u001b[K    94% |██████████████████████████████▏ | 931kB 74.5MB/s eta 0:00:01\r\u001b[K    95% |██████████████████████████████▌ | 942kB 74.7MB/s eta 0:00:01\r\u001b[K    96% |██████████████████████████████▉ | 952kB 45.2MB/s eta 0:00:01\r\u001b[K    97% |███████████████████████████████▏| 962kB 49.5MB/s eta 0:00:01\r\u001b[K    98% |███████████████████████████████▌| 972kB 51.0MB/s eta 0:00:01\r\u001b[K    99% |███████████████████████████████▉| 983kB 51.5MB/s eta 0:00:01\r\u001b[K    100% |████████████████████████████████| 993kB 20.8MB/s \n",
            "\u001b[?25h  Building wheel for PyDrive (setup.py) ... \u001b[?25ldone\n",
            "\u001b[K    100% |████████████████████████████████| 266kB 20.7MB/s \n",
            "\u001b[?25h  Building wheel for glove-python (setup.py) ... \u001b[?25ldone\n",
            "\u001b[?25h[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/words.zip.\n",
            "[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "x6geO8DrWm46",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Install Libraries"
      ]
    },
    {
      "metadata": {
        "id": "uzDf-6GqDCUG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "c8461eac-4c6b-4f1a-86ec-3a3dc028c9bc"
      },
      "cell_type": "code",
      "source": [
        "from itertools import *\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk import word_tokenize          \n",
        "from nltk.stem import WordNetLemmatizer \n",
        "\n",
        "from nltk.corpus import wordnet\n",
        "\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib\n",
        "%matplotlib inline\n",
        " \n",
        "import seaborn as sns\n",
        "sns.set_style(\"darkgrid\")\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.manifold import TSNE\n",
        "from scipy.sparse import csr_matrix\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import precision_score, recall_score, make_scorer\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "from sklearn import svm\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import tree\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "import seaborn as sns\n",
        "\n",
        "from wordcloud import WordCloud\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.naive_bayes import BernoulliNB, MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "from imblearn.under_sampling import ClusterCentroids\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "# from sklearn.grid_search import GridSearchCV\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from imblearn.under_sampling import NearMiss, RandomUnderSampler\n",
        "\n",
        "from keras.preprocessing import sequence\n",
        "from keras.utils import np_utils\n",
        "from keras.layers.core import Dense, Dropout, Activation, Lambda\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from collections import defaultdict\n",
        "from keras.layers.convolutional import Convolution1D\n",
        "from keras import backend as K\n",
        "from keras.models import Sequential\n",
        "from keras.layers.recurrent import LSTM, SimpleRNN, GRU\n",
        "\n",
        "import logging\n",
        "from gensim.models import word2vec\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "#importing the glove library\n",
        "from glove import Corpus, Glove\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "/usr/local/lib/python3.6/dist-packages/smart_open/ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
            "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "cEw-5famWt8i",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Data Pre-Processing"
      ]
    },
    {
      "metadata": {
        "id": "w6xJUYU6YhBk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "1. Balance Positive and Negative Reviews (Split Equally the Distribution into Positive and Negative Reviews)\n",
        "Metric: \n",
        "1,2 Rating - Negative Review\n",
        "4,5 Rating - Good Review\n",
        "Ignore 3 Rating as its Neutral"
      ]
    },
    {
      "metadata": {
        "id": "zQsQlxKexN_G",
        "colab_type": "code",
        "outputId": "e51cc8ec-a8f0-4110-a7e4-770821dcc781",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        }
      },
      "cell_type": "code",
      "source": [
        "link='https://drive.google.com/open?id=1Z_y9nw9nQCmuQUJyFUF2DkVw_7KX346b'\n",
        "fluff, id = link.split('=')\n",
        "print (id) # Verify that you have everything after '='\n",
        "downloaded = drive.CreateFile({'id':id}) \n",
        "downloaded.GetContentFile('reviews_Baby_5.json')  \n",
        "df = pd.read_json('reviews_Baby_5.json', orient='columns',lines=True)\n",
        "# Dataset is now stored in a Pandas Dataframe\n",
        "\n",
        "df.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1Z_y9nw9nQCmuQUJyFUF2DkVw_7KX346b\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>asin</th>\n",
              "      <th>helpful</th>\n",
              "      <th>overall</th>\n",
              "      <th>reviewText</th>\n",
              "      <th>reviewTime</th>\n",
              "      <th>reviewerID</th>\n",
              "      <th>reviewerName</th>\n",
              "      <th>summary</th>\n",
              "      <th>unixReviewTime</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>097293751X</td>\n",
              "      <td>[0, 0]</td>\n",
              "      <td>5</td>\n",
              "      <td>Perfect for new parents. We were able to keep ...</td>\n",
              "      <td>07 16, 2013</td>\n",
              "      <td>A1HK2FQW6KXQB2</td>\n",
              "      <td>Amanda Johnsen \"Amanda E. Johnsen\"</td>\n",
              "      <td>Awesine</td>\n",
              "      <td>1373932800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>097293751X</td>\n",
              "      <td>[0, 0]</td>\n",
              "      <td>5</td>\n",
              "      <td>This book is such a life saver.  It has been s...</td>\n",
              "      <td>06 29, 2013</td>\n",
              "      <td>A19K65VY14D13R</td>\n",
              "      <td>angela</td>\n",
              "      <td>Should be required for all new parents!</td>\n",
              "      <td>1372464000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>097293751X</td>\n",
              "      <td>[0, 0]</td>\n",
              "      <td>5</td>\n",
              "      <td>Helps me know exactly how my babies day has go...</td>\n",
              "      <td>03 19, 2014</td>\n",
              "      <td>A2LL1TGG90977E</td>\n",
              "      <td>Carter</td>\n",
              "      <td>Grandmother watching baby</td>\n",
              "      <td>1395187200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>097293751X</td>\n",
              "      <td>[0, 0]</td>\n",
              "      <td>5</td>\n",
              "      <td>I bought this a few times for my older son and...</td>\n",
              "      <td>08 17, 2013</td>\n",
              "      <td>A5G19RYX8599E</td>\n",
              "      <td>cfpurplerose</td>\n",
              "      <td>repeat buyer</td>\n",
              "      <td>1376697600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>097293751X</td>\n",
              "      <td>[0, 0]</td>\n",
              "      <td>4</td>\n",
              "      <td>I wanted an alternative to printing out daily ...</td>\n",
              "      <td>04 1, 2014</td>\n",
              "      <td>A2496A4EWMLQ7</td>\n",
              "      <td>C. Jeter</td>\n",
              "      <td>Great</td>\n",
              "      <td>1396310400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         asin helpful  overall  \\\n",
              "0  097293751X  [0, 0]        5   \n",
              "1  097293751X  [0, 0]        5   \n",
              "2  097293751X  [0, 0]        5   \n",
              "3  097293751X  [0, 0]        5   \n",
              "4  097293751X  [0, 0]        4   \n",
              "\n",
              "                                          reviewText   reviewTime  \\\n",
              "0  Perfect for new parents. We were able to keep ...  07 16, 2013   \n",
              "1  This book is such a life saver.  It has been s...  06 29, 2013   \n",
              "2  Helps me know exactly how my babies day has go...  03 19, 2014   \n",
              "3  I bought this a few times for my older son and...  08 17, 2013   \n",
              "4  I wanted an alternative to printing out daily ...   04 1, 2014   \n",
              "\n",
              "       reviewerID                        reviewerName  \\\n",
              "0  A1HK2FQW6KXQB2  Amanda Johnsen \"Amanda E. Johnsen\"   \n",
              "1  A19K65VY14D13R                              angela   \n",
              "2  A2LL1TGG90977E                              Carter   \n",
              "3   A5G19RYX8599E                        cfpurplerose   \n",
              "4   A2496A4EWMLQ7                            C. Jeter   \n",
              "\n",
              "                                   summary  unixReviewTime  \n",
              "0                                  Awesine      1373932800  \n",
              "1  Should be required for all new parents!      1372464000  \n",
              "2                Grandmother watching baby      1395187200  \n",
              "3                             repeat buyer      1376697600  \n",
              "4                                    Great      1396310400  "
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "moDBRsFD_S0I",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Remove any 'neutral' ratings equal to 3\n",
        "df = df[df['overall'] != 3]\n",
        "\n",
        "# # df = pd.concat(f1)\n",
        "# df = df.sample(frac=0.7, random_state=0) #uncomment to use full set of data\n",
        "# # Drop missing values\n",
        "df.dropna(inplace=True)\n",
        "\n",
        "# Encode 4s and 5s as 1 (positive sentiment) and 1s and 2s as 0 (negative sentiment)\n",
        "df['Sentiment'] = np.where(df['overall'] > 3, 1, 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-wLY1ly41R0m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# df = df.sort_values(by=['overall'])\n",
        "# # # df = df.head(98000)\n",
        "# dfnegative = df.head(16932)\n",
        "# dfpositive = df.iloc[16933:,:]\n",
        "# dfpositive4 = dfpositive.head(8466)\n",
        "# dfpositive5 = dfpositive.tail(8466)\n",
        "\n",
        "# f = [dfpositive4,dfpositive5]\n",
        "\n",
        "# dfpositives = pd.concat(f)\n",
        "\n",
        "# f1 = [dfnegative,dfpositives]\n",
        "\n",
        "# df = pd.concat(f1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9aocVpo2t99B",
        "colab_type": "code",
        "outputId": "6d38c7da-efba-4329-c490-cd1599129f61",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"The Number of Reviews less than rating 3\")\n",
        "df[df['overall'] < 3].shape"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Number of Reviews less than rating 3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(16932, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "metadata": {
        "id": "NneE8PLIt6Pb",
        "colab_type": "code",
        "outputId": "4592d728-ccc4-45a5-81bd-9f1fc72f83dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"The Number of Reviews greater than 3\")\n",
        "df[df['overall'] > 3].shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Number of Reviews greater than 3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(125348, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "PiZ3Ue_YBzDl",
        "colab_type": "code",
        "outputId": "6698843f-0f64-4d40-f851-5e554badf0d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        }
      },
      "cell_type": "code",
      "source": [
        "print(\"The Size of Dataset\",df.shape)\n",
        "print('Distribution of Positive and Negative Reviews, Three being the threshold')\n",
        "df.hist('overall')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The Size of Dataset (142280, 10)\n",
            "Distribution of Positive and Negative Reviews, Three being the threshold\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[<matplotlib.axes._subplots.AxesSubplot object at 0x7f94e5531d30>]],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYcAAAEICAYAAAC0+DhzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAY/g/9TqgHkEcnHAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "4tWQufRmCGg0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X = df['reviewText']\n",
        "y = df['Sentiment']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "009IjM2SZHC5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Pre-Process the Review Text"
      ]
    },
    {
      "metadata": {
        "id": "afMlpExZZP-d",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Stemming, Remove Punctuation, Remove Non-Characters, HTML Tags,Remove Stop Words"
      ]
    },
    {
      "metadata": {
        "id": "UsoSUlSm-dOw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def cleanText(raw_text, remove_stopwords=True, stemming=False, split_text=False, \\\n",
        "             ):\n",
        "    '''\n",
        "    Convert a raw review to a cleaned review\n",
        "    '''\n",
        "    text = BeautifulSoup(raw_text, 'lxml').get_text()  #remove html\n",
        "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", text)  # remove non-character\n",
        "    words = letters_only.lower().split() # convert to lower case \n",
        "    \n",
        "    if remove_stopwords: # remove stopword\n",
        "        stops = set(stopwords.words(\"english\"))\n",
        "        stops.remove('not')\n",
        "        stops.remove('no')\n",
        "        words = [w for w in words if not w in stops]\n",
        "    if stemming==True: # stemming\n",
        "        stemmer = PorterStemmer()\n",
        "        stemmer = SnowballStemmer('english') \n",
        "        words = [stemmer.stem(w) for w in words]\n",
        "        \n",
        "    if split_text==True:  # split text\n",
        "        return (words)\n",
        "    \n",
        "    return( \" \".join(words))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NmrBUUPKDIwx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_cleaned = []\n",
        "    \n",
        "for d in X:\n",
        "    X_cleaned.append(cleanText(d))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eAIyoB_-fG9g",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Classification using CountVectorizer Bag of Words"
      ]
    },
    {
      "metadata": {
        "id": "G6MI86B8A1cF",
        "colab_type": "code",
        "outputId": "d54601eb-562b-48de-9369-9b8e2ab10c3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        }
      },
      "cell_type": "code",
      "source": [
        "countVect = CountVectorizer(min_df = 50, ngram_range = (1,2),strip_accents='unicode', binary=True)\n",
        "X_all_countVect = countVect.fit_transform(X_cleaned)\n",
        "\n",
        "print(\"Number of features : %d \\n\" %len(countVect.get_feature_names())) #1722\n",
        "print(\"Show some feature names : \\n\", countVect.get_feature_names()[::1000])\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of features : 18179 \n",
            "\n",
            "Show some feature names : \n",
            " ['aa', 'baby nice', 'bought year', 'component', 'discount', 'fall apart', 'get without', 'highly recommend', 'let us', 'makes difference', 'nice feature', 'opinion not', 'process', 'right around', 'size great', 'straps also', 'toddler', 'using potty', 'would not']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bU_F1HiGCJW9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "knn = []\n",
        "svm = []\n",
        "dt =[]\n",
        "nb =[]\n",
        "lr =[]\n",
        "rus = RandomUnderSampler(random_state=777)\n",
        "X_RUS, y_RUS = rus.fit_sample(X_all_countVect, y)\n",
        "target_names = ['Positive','Negative']\n",
        "skf = StratifiedKFold(n_splits=5, random_state=None, shuffle=False)\n",
        "for train_index, test_index in skf.split(X_RUS, y_RUS):\n",
        "  X_train_countVect = X_RUS[train_index]\n",
        "  y_train = y_RUS[train_index]\n",
        "  X_test_countVect = X_RUS[test_index]\n",
        "  y_test = y_RUS[test_index]\n",
        "  \n",
        "  \n",
        "  knn_mean = knn_classifier(X_train_countVect,y_train,X_test_countVect,y_test,target_names)\n",
        "  knn.append(knn_mean)\n",
        "  dt_mean = dt_classifier(X_train_countVect,y_train,X_test_countVect,y_test,target_names)\n",
        "  dt.append(dt_mean)\n",
        "  nb_mean = nb_classifier(X_train_countVect,y_train,X_test_countVect,y_test,target_names)\n",
        "  nb.append(nb_mean)\n",
        "  lr_mean = lr_classifier(X_train_countVect,y_train,X_test_countVect,y_test,target_names)\n",
        "  lr.append(lr_mean)\n",
        "  svm_mean = svc_classifier(X_train_countVect,y_train,X_test_countVect,y_test,target_names)\n",
        "  svm.append(svm_mean)\n",
        "  \n",
        "  \n",
        "  print('The Accuracy for KNN:',sum(knn)/len(knn))\n",
        "  print('The Accuracy for SVM:',sum(svm)/len(svm))\n",
        "  print('The Accuracy for DT:',sum(dt)/len(dt))\n",
        "  print('The Accuracy for MNB:',sum(nb)/len(nb))\n",
        "  print('The Accuracy for LR:',sum(lr)/len(lr))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "a-BTLkPfBFdO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Classification Using Tf-IDF BoW Model"
      ]
    },
    {
      "metadata": {
        "id": "5LsiPiM3BEK0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Fit and transform the training data to a document-term matrix using TfidfVectorizer \n",
        "tfidf = TfidfVectorizer(min_df = 5, ngram_range = (1,2),strip_accents='unicode', binary=True,max_features=5000) #minimum document frequency of 5\n",
        "X_all_tfidf = tfidf.fit_transform(X_cleaned)\n",
        "print(\"Number of features : %d \\n\" %len(tfidf.get_feature_names())) #1722\n",
        "print(\"Show some feature names : \\n\", tfidf.get_feature_names()[::1000])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4z_ImC_AN8F-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "knn = []\n",
        "svm = []\n",
        "dt =[]\n",
        "nb =[]\n",
        "lr =[]\n",
        "rus = RandomUnderSampler(random_state=777)\n",
        "X_RUS, y_RUS = rus.fit_sample(X_all_tfidf, y)\n",
        "target_names = ['Positive','Negative']\n",
        "skf = StratifiedKFold(n_splits=5, random_state=None, shuffle=False)\n",
        "for train_index, test_index in skf.split(X_RUS, y_RUS):\n",
        "  X_train_countVect = X_RUS[train_index]\n",
        "  y_train = y_RUS[train_index]\n",
        "  X_test_countVect = X_RUS[test_index]\n",
        "  y_test = y_RUS[test_index]\n",
        "  \n",
        "  \n",
        "  knn_mean = knn_classifier(X_train_countVect,y_train,X_test_countVect,y_test,target_names)\n",
        "  knn.append(knn_mean)\n",
        "  dt_mean = dt_classifier(X_train_countVect,y_train,X_test_countVect,y_test,target_names)\n",
        "  dt.append(dt_mean)\n",
        "  nb_mean = nb_classifier(X_train_countVect,y_train,X_test_countVect,y_test,target_names)\n",
        "  nb.append(nb_mean)\n",
        "  lr_mean = lr_classifier(X_train_countVect,y_train,X_test_countVect,y_test,target_names)\n",
        "  lr.append(lr_mean)\n",
        "  svm_mean = svc_classifier(X_train_countVect,y_train,X_test_countVect,y_test,target_names)\n",
        "  svm.append(svm_mean)\n",
        "  \n",
        "  \n",
        "  print('The Accuracy for KNN:',sum(knn)/len(knn))\n",
        "  print('The Accuracy for SVM:',sum(svm)/len(svm))\n",
        "  print('The Accuracy for DT:',sum(dt)/len(dt))\n",
        "  print('The Accuracy for MNB:',sum(nb)/len(nb))\n",
        "  print('The Accuracy for LR:',sum(lr)/len(lr))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "9gEceKB2FytS"
      },
      "cell_type": "markdown",
      "source": [
        "# **K-Nearest Neighbours**"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "tB-DTx6WFytP",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def knn_classifier(X_train_countVect,y_train,X_test_countVect,y_test,target): \n",
        "    classifier=KNeighborsClassifier(n_neighbors=5)\n",
        "    classifier.fit(X_train_countVect,y_train)\n",
        "\n",
        "    y_pred=classifier.predict(X_test_countVect)\n",
        "\n",
        "    y_pred_train = classifier.predict(X_train_countVect)\n",
        "    #Uncomment for Detailed Report\n",
        "#     print('KNN Results:')\n",
        "#     print(\"KNN Accuracy:\",metrics.accuracy_score(y_test,y_pred))\n",
        "#     print(classification_report(y_test, y_pred))\n",
        "#     print(\"Confusion Matrix\",confusion_matrix(y_test, y_pred))    \n",
        "#     print(\"KNN Train Accuracy:\",metrics.accuracy_score(y_train,y_pred_train))\n",
        "#     print(classification_report(y_train, y_pred_train))\n",
        "    \n",
        "    return metrics.accuracy_score(y_test,y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Mu5w-hFSFytO"
      },
      "cell_type": "markdown",
      "source": [
        "# SVM"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "6j8v5PNAFytL",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#SVM\n",
        "def svc_classifier(X_train_countVect,y_train,X_test_countVect,y_test,target_names): \n",
        "  from sklearn import svm\n",
        "  clf=svm.SVC(kernel='linear')\n",
        "  clf.fit(X_train_countVect,y_train)\n",
        "\n",
        "  y_pred=clf.predict(X_test_countVect)\n",
        "  \n",
        "  y_pred_train =clf.predict(X_train_countVect)\n",
        "\n",
        "\n",
        "  #Uncomment for Detailed Report\n",
        "#   print('SVM Results:')\n",
        "#   print(\"SVM Accuracy:\",metrics.accuracy_score(y_test,y_pred))\n",
        "#   print(classification_report(y_test, y_pred, target_names=target_names))\n",
        "#   print(\"Confusion Matrix\",confusion_matrix(y_test, y_pred))    \n",
        "#   print(\"SVM Train Accuracy:\",metrics.accuracy_score(y_train,y_pred_train))\n",
        "#   print(classification_report(y_train, y_pred_train, target_names=target_names))\n",
        "\n",
        "  return metrics.accuracy_score(y_test,y_pred)\n",
        "  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Wb9f9xhVFytJ"
      },
      "cell_type": "markdown",
      "source": [
        "# MultiNomial NB"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "QAaFPSLBFytD",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Naive Bayes classifier\n",
        "def nb_classifier(X_train_countVect,y_train,X_test_countVect,y_test,target_names): \n",
        "\n",
        "  clf = MultinomialNB()\n",
        "  clf.fit(X_train_countVect.toarray(),y_train)\n",
        "\n",
        "  y_pred=clf.predict(X_test_countVect)\n",
        "\n",
        "  y_pred_train =clf.predict(X_train_countVect)\n",
        "   #Uncomment for Detailed Report\n",
        "#   print('NB Results:')\n",
        "#   print(\"MNB Accuracy:\",metrics.accuracy_score(y_test,y_pred))\n",
        "#   print(classification_report(y_test, y_pred, target_names=target_names))\n",
        "#   print(\"Confusion Matrix\",confusion_matrix(y_test, y_pred))    \n",
        "#   print(\"MNB Train Accuracy:\",metrics.accuracy_score(y_train,y_pred_train))\n",
        "#   print(classification_report(y_train, y_pred_train, target_names=target_names))\n",
        "\n",
        "  return metrics.accuracy_score(y_test,y_pred)\n",
        "  \n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "2i4xUBtNFytB"
      },
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "w_k-BZGrFys0",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Logistic Regression\n",
        "def lr_classifier(X_train_countVect,y_train,X_test_countVect,y_test,target_names): \n",
        "  lr = LogisticRegression()\n",
        "  lr.fit(X_train_countVect.toarray(), y_train)\n",
        "\n",
        "\n",
        "  y_pred=lr.predict(X_test_countVect)\n",
        "\n",
        "  y_pred_train =lr.predict(X_train_countVect)\n",
        "   #Uncomment for Detailed Report\n",
        "#   print('LR Results:')\n",
        "#   print(\"LR Accuracy:\",metrics.accuracy_score(y_test,y_pred))\n",
        "#   print(classification_report(y_test, y_pred, target_names=target_names))\n",
        "#   print(\"Confusion Matrix\",confusion_matrix(y_test, y_pred))    \n",
        "#   print(\"LR Train Accuracy:\",metrics.accuracy_score(y_train,y_pred_train))\n",
        "#   print(classification_report(y_train, y_pred_train, target_names=target_names))\n",
        "  \n",
        "  return metrics.accuracy_score(y_test,y_pred)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "yERFIF_pFysg"
      },
      "cell_type": "markdown",
      "source": [
        "# Adaboosting (Decision Trees)"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "N28PY3zrFysQ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# #   Decision Trees\n",
        "def dt_classifier(X_train_countVect,y_train,X_test_countVect,y_test,target_names): \n",
        "  clf = AdaBoostClassifier(n_estimators=400,learning_rate=1)\n",
        "  clf.fit(X_train_countVect,y_train)\n",
        "  \n",
        "  y_pred=clf.predict(X_test_countVect)\n",
        "  \n",
        "  y_pred_train =clf.predict(X_train_countVect)\n",
        " #Uncomment for Detailed Report\n",
        "\n",
        "#   print('Adaboosting Results:')\n",
        "#   print(\"Adaboosting DT Accuracy:\",metrics.accuracy_score(y_test,y_pred))\n",
        "#   print(classification_report(y_test, y_pred, target_names=target_names))\n",
        "#   print(\"Confusion Matrix\",confusion_matrix(y_test, y_pred))    \n",
        "#   print(\"Adaboosting DT Train Accuracy:\",metrics.accuracy_score(y_train,y_pred_train))\n",
        "#   print(classification_report(y_train, y_pred_train, target_names=target_names))\n",
        "  \n",
        "  return metrics.accuracy_score(y_test,y_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_3NBtGgtUdai",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# LSTM with GLOVE\n"
      ]
    },
    {
      "metadata": {
        "id": "WPJPu_uHGNeX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Split review text into parsed sentences uisng NLTK's punkt tokenizer\n",
        "# nltk.download()\n",
        "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "\n",
        "def parseSent(review, tokenizer, remove_stopwords=False):\n",
        "    '''\n",
        "    Parse text into sentences\n",
        "    '''\n",
        "    raw_sentences = tokenizer.tokenize(review.strip())\n",
        "    sentences = []\n",
        "    for raw_sentence in raw_sentences:\n",
        "        if len(raw_sentence) > 0:\n",
        "            sentences.append(cleanText(raw_sentence, remove_stopwords, split_text=True))\n",
        "    return sentences\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "L9EUSwHraAw3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_cleaned=np.array(X_cleaned)\n",
        "y=np.array(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3FYFtaMgXIPO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Parse each review in the training set into sentences\n",
        "sentences = []\n",
        "for review in X_cleaned:\n",
        "  sentences += parseSent(review, tokenizer)\n",
        "\n",
        "# creating a corpus object\n",
        "  corpus = Corpus() \n",
        "  #training the corpus to generate the co occurence matrix which is used in GloVe\n",
        "  corpus.fit(sentences, window=10)\n",
        "  #creating a Glove object which will use the matrix created in the above lines to create embeddings\n",
        "  #We can set the learning rate as it uses Gradient Descent and number of components\n",
        "  glove = Glove(no_components=300, learning_rate=0.05)\n",
        " \n",
        "  glove.fit(corpus.matrix, epochs=3, no_threads=4, verbose=True)\n",
        "  glove.add_dictionary(corpus.dictionary)\n",
        "  glove.save('glove.model') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9PkAF6J_X15b",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "w2v = glove.load(\"glove.model\")\n",
        "w2v1 = glove.load(\"glove.model\")\n",
        "print(\"Vector Representation for the word Perfect in Glove:\")\n",
        "np.array(w2v.word_vectors[w2v.dictionary['perfect']]).reshape(1,300)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gDoKLWLcTiD-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "skf = StratifiedKFold(n_splits=5, random_state=None, shuffle=False)\n",
        "\n",
        "\n",
        "maxlen = 100\n",
        "batch_size = 64\n",
        "nb_classes = 2\n",
        "nb_epoch = 3\n",
        "\n",
        "\n",
        "num_features = 300  #embedding dimension                     \n",
        "min_word_count = 10                \n",
        "num_workers = 4       \n",
        "context = 10                                                                                          \n",
        "downsampling = 1e-3 \n",
        "\n",
        "\n",
        "\n",
        "for train_index, test_index in skf.split(X_cleaned,y):\n",
        "  X_train = X_cleaned[train_index]\n",
        "  y_train = y[train_index]\n",
        "  X_test = X_cleaned[test_index]\n",
        "  y_test = y[test_index]\n",
        "  \n",
        "  # Split review text into parsed sentences uisng NLTK's punkt tokenizer\n",
        "  tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
        "\n",
        "\n",
        "  # Parse each review in the training set into sentences\n",
        "  sentences = []\n",
        "  for review in X_train:\n",
        "    sentences += parseSent(review, tokenizer)\n",
        " \n",
        "\n",
        "  print(\"Training glove model ...\\n\")\n",
        "  # creating a corpus object\n",
        "  corpus = Corpus() \n",
        "  #training the corpus to generate the co occurence matrix which is used in GloVe\n",
        "  corpus.fit(sentences, window=10)\n",
        "  #creating a Glove object which will use the matrix created in the above lines to create embeddings\n",
        "  #We can set the learning rate as it uses Gradient Descent and number of components\n",
        "  glove = Glove(no_components=300, learning_rate=0.05)\n",
        " \n",
        "  glove.fit(corpus.matrix, epochs=5, no_threads=4, verbose=True)\n",
        "  glove.add_dictionary(corpus.dictionary)\n",
        "  glove.save('glove.model')\n",
        "  w2v = glove.load(\"glove.model\")\n",
        "\n",
        "\n",
        "#   # Get glove embedding matrix\n",
        "  embedding_matrix = w2v.word_vectors  # embedding matrix, type = numpy.ndarray \n",
        "  top_words = w2v.word_vectors.shape[0] #4016\n",
        "\n",
        "    \n",
        "#   # Vectorize X_train and X_test to 2D tensor\n",
        "  \n",
        "  tokenizer = Tokenizer(nb_words=top_words) #only consider top 20000 words in the corpse\n",
        "  tokenizer.fit_on_texts(X_train)\n",
        "  tokenizer.word_index #access word-to-index dictionary of trained tokenizer\n",
        "\n",
        "  sequences_train = tokenizer.texts_to_sequences(X_train)\n",
        "  sequences_test = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "  X_train_seq = sequence.pad_sequences(sequences_train, maxlen=maxlen)\n",
        "  X_test_seq = sequence.pad_sequences(sequences_test, maxlen=maxlen)\n",
        "\n",
        "\n",
        "   # one-hot encoding of y_train and y_test\n",
        "  y_train_seq = np_utils.to_categorical(y_train, nb_classes)\n",
        "  y_test_seq = np_utils.to_categorical(y_test, nb_classes)\n",
        "\n",
        "  print('X_train shape:', X_train_seq.shape) #(27799, 100)\n",
        "  print('X_test shape:', X_test_seq.shape) #(3089, 100)\n",
        "  print('y_train shape:', y_train_seq.shape) #(27799, 2)\n",
        "  print('y_test shape:', y_test_seq.shape) #(3089, 2)\n",
        "\n",
        "  # Construct glove embedding layer\n",
        "  embedding_layer = Embedding(embedding_matrix.shape[0], #4016\n",
        "                            embedding_matrix.shape[1], #300\n",
        "                            weights=[embedding_matrix])\n",
        "  # Construct LSTM with Word2Vec embedding\n",
        "  model2 = Sequential()\n",
        "  model2.add(embedding_layer)\n",
        "  model2.add(LSTM(200, dropout_W=0.25, dropout_U=0.25))\n",
        "  model2.add(Dense(nb_classes))\n",
        "  model2.add(Activation('relu'))\n",
        "  model2.summary()\n",
        "\n",
        "  # Compile model\n",
        "  model2.compile(loss='binary_crossentropy',\n",
        "              optimizer='nadam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  model2.fit(X_train_seq, y_train_seq, batch_size=batch_size, nb_epoch=3, verbose=1)\n",
        "\n",
        "\n",
        "  # Model evaluation\n",
        "  score = model2.evaluate(X_test_seq, y_test_seq, batch_size=batch_size)\n",
        "  nb.append(score)\n",
        "  print('Test accuracy : {:.4f}'.format(score[1]))\n",
        "\n",
        "  print('Test loss : {:.4f}'.format(score[0]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EdZwiPLgsjcH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#LSTM with Word2vec\n"
      ]
    },
    {
      "metadata": {
        "id": "aY9Gp1KdIZgW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import logging\n",
        "from gensim.models import word2vec\n",
        "from gensim.models import Word2Vec\n",
        "from gensim.models.keyedvectors import KeyedVectors\n",
        "\n",
        "\n",
        "# Fit parsed sentences to Word2Vec model \n",
        "# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',level=logging.INFO)\n",
        "\n",
        "num_features = 300  #embedding dimension                     \n",
        "min_word_count = 10                \n",
        "num_workers = 4       \n",
        "context = 10                                                                                          \n",
        "downsampling = 1e-3 \n",
        "\n",
        "print(\"Training Word2Vec model ...\\n\")\n",
        "w2v = Word2Vec(sentences, workers=num_workers, size=num_features, min_count = min_word_count,\\\n",
        "                 window = context, sample = downsampling)\n",
        "w2v.init_sims(replace=True)\n",
        "w2v.save(\"w2v_300features_10minwordcounts_10context\") #save trained word2vec model\n",
        "\n",
        "print(\"Number of words in the vocabulary list : %d \\n\" %len(w2v.wv.index2word)) #4016 \n",
        "print(\"Show first 10 words in the vocalbulary list  vocabulary list: \\n\", w2v.wv.index2word[0:10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wi6sipyl048u",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Load trained Word2Vec model\n",
        "w2v = Word2Vec.load(\"w2v_300features_10minwordcounts_10context\")\n",
        "\n",
        "\n",
        "# Get Word2Vec embedding matrix\n",
        "embedding_matrix = w2v.wv.syn0  # embedding matrix, type = numpy.ndarray \n",
        "print(\"Shape of embedding matrix : \", embedding_matrix.shape) #(4016, 300) = (volcabulary size, embedding dimension)\n",
        "# w2v.wv.syn0[0] #feature vector of the first word in the volcabulary list\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tz6ezIvQA-Ps",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "top_words = embedding_matrix.shape[0] #4016\n",
        "maxlen = 100  \n",
        "batch_size = 32\n",
        "nb_classes = 2\n",
        "nb_epoch = 3\n",
        "\n",
        "\n",
        "# Vectorize X_train and X_test to 2D tensor\n",
        "tokenizer = Tokenizer(nb_words=top_words) #only consider top 20000 words in the corpse\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "# tokenizer.word_index #access word-to-index dictionary of trained tokenizer\n",
        "\n",
        "sequences_train = tokenizer.texts_to_sequences(X_train)\n",
        "sequences_test = tokenizer.texts_to_sequences(X_test)\n",
        "\n",
        "X_train_seq = sequence.pad_sequences(sequences_train, maxlen=maxlen)\n",
        "X_test_seq = sequence.pad_sequences(sequences_test, maxlen=maxlen)\n",
        "\n",
        "\n",
        "# one-hot encoding of y_train and y_test\n",
        "y_train_seq = np_utils.to_categorical(y_train, nb_classes)\n",
        "y_test_seq = np_utils.to_categorical(y_test, nb_classes)\n",
        "\n",
        "print('X_train shape:', X_train_seq.shape) #(27799, 100)\n",
        "print('X_test shape:', X_test_seq.shape) #(3089, 100)\n",
        "print('y_train shape:', y_train_seq.shape) #(27799, 2)\n",
        "print('y_test shape:', y_test_seq.shape) #(3089, 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FJ6zI9gg9hF9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# Construct Word2Vec embedding layer\n",
        "embedding_layer = Embedding(embedding_matrix.shape[0], #4016\n",
        "                            embedding_matrix.shape[1], #300\n",
        "                            weights=[embedding_matrix])\n",
        "\n",
        "\n",
        "# Construct LSTM with Word2Vec embedding\n",
        "model2 = Sequential()\n",
        "model2.add(embedding_layer)\n",
        "model2.add(LSTM(128, dropout_W=0.2, dropout_U=0.2)) \n",
        "model2.add(Dense(nb_classes))\n",
        "model2.add(Activation('softmax'))\n",
        "model2.summary()\n",
        "\n",
        "# Compile model\n",
        "model2.compile(loss='binary_crossentropy',\n",
        "              optimizer='adam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model2.fit(X_train_seq, y_train_seq, batch_size=batch_size, nb_epoch=nb_epoch, verbose=1)\n",
        "\n",
        "\n",
        "# Model evaluation\n",
        "score = model2.evaluate(X_test_seq, y_test_seq, batch_size=batch_size)\n",
        "print('Test loss : {:.4f}'.format(score[0]))\n",
        "print('Test accuracy : {:.4f}'.format(score[1]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2C-852aV6ABq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Data Visualization"
      ]
    },
    {
      "metadata": {
        "id": "sS0wsjfKEEFY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# T-SNE with Glove"
      ]
    },
    {
      "metadata": {
        "id": "BTzMQOdsG6lW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def tsnescatterplot(model, word, list_names):\n",
        "    \"\"\" Plot in seaborn the results from the t-SNE dimensionality reduction algorithm of the vectors of a query word,\n",
        "    its list of most similar words, and a list of words.\n",
        "    \"\"\"\n",
        "    arrays = np.empty((0, 300), dtype='f')\n",
        "    word_labels = [word]\n",
        "    color_list  = ['red']\n",
        "\n",
        "    # adds the vector of the query word\n",
        "    arrays = np.append(arrays, np.array(model.word_vectors[model.dictionary[word]]).reshape(1,300), axis=0)\n",
        "    \n",
        "    # gets list of most similar words\n",
        "    close_words = model.most_similar(word)\n",
        "    \n",
        "    # adds the vector for each of the closest words to the array\n",
        "    for wrd_score in close_words:\n",
        "        wrd_vector = np.array(model.word_vectors[model.dictionary[wrd_score[0]]]).reshape(1,300)\n",
        "        word_labels.append(wrd_score[0])\n",
        "        color_list.append('blue')\n",
        "        arrays = np.append(arrays, wrd_vector, axis=0)\n",
        "    # adds the vector for each of the words from list_names to the array\n",
        "    for wrd in list_names:\n",
        "        wrd_vector = np.array(model.word_vectors[model.dictionary[wrd]]).reshape(1,300)\n",
        "        word_labels.append(wrd)\n",
        "        color_list.append('green')\n",
        "        arrays = np.append(arrays, wrd_vector, axis=0)\n",
        "        \n",
        "    # Reduces the dimensionality from 300 to 50 dimensions with PCA\n",
        "    reduc = PCA(n_components=10).fit_transform(arrays)\n",
        "    \n",
        "    # Finds t-SNE coordinates for 2 dimensions\n",
        "    np.set_printoptions(suppress=True)\n",
        "    \n",
        "    Y = TSNE(n_components=2, random_state=0, perplexity=15).fit_transform(reduc)\n",
        "    \n",
        "    # Sets everything up to plot\n",
        "    df = pd.DataFrame({'x': [x for x in Y[:, 0]],\n",
        "                       'y': [y for y in Y[:, 1]],\n",
        "                       'words': word_labels,\n",
        "                       'color': color_list})\n",
        "    fig, _ = plt.subplots()\n",
        "    fig.set_size_inches(9, 9)\n",
        "    \n",
        "    # Basic plot\n",
        "    p1 = sns.regplot(data=df,\n",
        "                     x=\"x\",\n",
        "                     y=\"y\",\n",
        "                     fit_reg=False,\n",
        "                     marker=\"o\",\n",
        "                     scatter_kws={'s': 40,\n",
        "                                  'facecolors': df['color']\n",
        "                                 }\n",
        "                    )\n",
        "    \n",
        "    # Adds annotations one by one with a loop\n",
        "    for line in range(0, df.shape[0]):\n",
        "         p1.text(df[\"x\"][line],\n",
        "                 df['y'][line],\n",
        "                 '  ' + df[\"words\"][line].title(),\n",
        "                 horizontalalignment='left',\n",
        "                 verticalalignment='bottom', size='medium',\n",
        "                 color=df['color'][line],\n",
        "                 weight='normal'\n",
        "                ).set_size(15)\n",
        "\n",
        "    \n",
        "    plt.xlim(Y[:, 0].min()-50, Y[:, 0].max()+50)\n",
        "    plt.ylim(Y[:, 1].min()-50, Y[:, 1].max()+50)\n",
        "            \n",
        "    plt.title('t-SNE visualization for {}'.format(word.title()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6KjtPIujG_Ax",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(\"Glove Model Represenation of Love:\")\n",
        "tsnescatterplot(w2v1, 'love', ['money', 'excellent', 'cheap', 'perfect', 'easy', 'affordable', 'experience'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5V_MpSvLHbdm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(\"Glove Model Represenation of Hate:\")\n",
        "tsnescatterplot(w2v1, 'hate', ['horrible','dislike','pathetic','poor','quality','disgusting','awful'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dv1FhX-QsoW1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# WordCloud"
      ]
    },
    {
      "metadata": {
        "id": "BmpBL_BMUFQg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Most Commonly Used Words in Negative Reviews\n",
        "#Most Commonly Used Words in Positive Reviews\n",
        "\n",
        "def create_word_cloud(sentiment):\n",
        "    try: \n",
        "#         df_brand = df.loc[df['Brand Name'].isin([brand])]\n",
        "        df_brand_sample = df.sample(frac=0.1)\n",
        "        word_cloud_collection = ''\n",
        "        \n",
        "        if sentiment == 1:\n",
        "            df_reviews = df_brand_sample[df_brand_sample[\"Sentiment\"]==1][\"reviewText\"]\n",
        "            \n",
        "        if sentiment == 0:\n",
        "            df_reviews = df_brand_sample[df_brand_sample[\"Sentiment\"]==0][\"reviewText\"]\n",
        "            \n",
        "        for val in df_reviews.str.lower():\n",
        "            tokens = nltk.word_tokenize(val)\n",
        "            tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
        "            for words in tokens:\n",
        "                word_cloud_collection = word_cloud_collection + words + ' '\n",
        "\n",
        "        wordcloud = WordCloud(max_font_size=50, width=500, height=300).generate(word_cloud_collection)\n",
        "        plt.figure(figsize=(20,20))\n",
        "        plt.imshow(wordcloud)\n",
        "        plt.axis(\"off\")\n",
        "        plt.show()\n",
        "    except: \n",
        "        pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xbKVbZyn9ASx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(\"Word Cloud from Positive Reviews\")\n",
        "create_word_cloud(1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JwYpWSOaKLDG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(\"Word Cloud from Negative Reviews\")\n",
        "create_word_cloud(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XGf3nMDrK6dS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}